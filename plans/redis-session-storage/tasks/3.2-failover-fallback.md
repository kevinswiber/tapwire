# Task 3.2: Failover & Fallback Implementation

## Objective
Implement robust failover mechanisms to handle Redis unavailability, including automatic fallback to in-memory storage, circuit breaker pattern, and health monitoring.

## Prerequisites
- Task 2.1 completed (RedisStore implementation)
- Understanding of circuit breaker pattern
- Health check requirements defined

## Deliverables

### 1. Circuit Breaker Implementation
Location: `shadowcat/src/session/storage/circuit_breaker.rs`

```rust
use std::sync::atomic::{AtomicU32, AtomicU64, Ordering};
use tokio::sync::RwLock;
use std::time::{Duration, Instant};

pub struct CircuitBreaker {
    state: Arc<RwLock<CircuitState>>,
    config: CircuitConfig,
    failure_count: AtomicU32,
    success_count: AtomicU32,
    last_failure_time: Arc<RwLock<Option<Instant>>>,
}

#[derive(Clone)]
pub struct CircuitConfig {
    pub failure_threshold: u32,      // Failures to open circuit
    pub success_threshold: u32,      // Successes to close circuit
    pub timeout: Duration,           // Time before trying half-open
    pub half_open_requests: u32,     // Requests allowed in half-open
}

#[derive(Debug, Clone)]
pub enum CircuitState {
    Closed,                    // Normal operation
    Open { since: Instant },   // Failing, reject requests
    HalfOpen { attempts: u32 }, // Testing recovery
}

impl CircuitBreaker {
    pub fn new(config: CircuitConfig) -> Self {
        Self {
            state: Arc::new(RwLock::new(CircuitState::Closed)),
            config,
            failure_count: AtomicU32::new(0),
            success_count: AtomicU32::new(0),
            last_failure_time: Arc::new(RwLock::new(None)),
        }
    }
    
    pub async fn call<F, T, E>(&self, f: F) -> Result<T, CircuitError<E>>
    where
        F: Future<Output = Result<T, E>>,
    {
        // Check if we should attempt the call
        if !self.should_attempt().await {
            return Err(CircuitError::Open);
        }
        
        // Execute the operation
        match f.await {
            Ok(result) => {
                self.on_success().await;
                Ok(result)
            }
            Err(e) => {
                self.on_failure().await;
                Err(CircuitError::CallFailed(e))
            }
        }
    }
    
    async fn should_attempt(&self) -> bool {
        let state = self.state.read().await;
        match *state {
            CircuitState::Closed => true,
            CircuitState::Open { since } => {
                // Check if timeout has elapsed
                if since.elapsed() >= self.config.timeout {
                    // Transition to half-open
                    drop(state);
                    let mut state = self.state.write().await;
                    *state = CircuitState::HalfOpen { attempts: 0 };
                    true
                } else {
                    false
                }
            }
            CircuitState::HalfOpen { attempts } => {
                attempts < self.config.half_open_requests
            }
        }
    }
    
    async fn on_success(&self) {
        self.success_count.fetch_add(1, Ordering::Relaxed);
        self.failure_count.store(0, Ordering::Relaxed);
        
        let mut state = self.state.write().await;
        match *state {
            CircuitState::HalfOpen { .. } => {
                if self.success_count.load(Ordering::Relaxed) >= self.config.success_threshold {
                    *state = CircuitState::Closed;
                    self.success_count.store(0, Ordering::Relaxed);
                }
            }
            _ => {}
        }
    }
    
    async fn on_failure(&self) {
        self.failure_count.fetch_add(1, Ordering::Relaxed);
        *self.last_failure_time.write().await = Some(Instant::now());
        
        let failures = self.failure_count.load(Ordering::Relaxed);
        if failures >= self.config.failure_threshold {
            let mut state = self.state.write().await;
            *state = CircuitState::Open { since: Instant::now() };
            self.failure_count.store(0, Ordering::Relaxed);
        }
    }
}
```

### 2. Fallback Storage Wrapper
Location: `shadowcat/src/session/storage/fallback.rs`

```rust
pub struct FallbackStore {
    primary: Arc<dyn SessionStore>,
    fallback: Arc<dyn SessionStore>,
    circuit_breaker: CircuitBreaker,
    health_monitor: HealthMonitor,
    config: FallbackConfig,
}

pub struct FallbackConfig {
    pub mode: FallbackMode,
    pub sync_on_recovery: bool,
    pub log_failures: bool,
    pub alert_on_fallback: bool,
}

pub enum FallbackMode {
    Immediate,        // Fallback immediately on any error
    AfterRetries(u32), // Fallback after N retries
    CircuitBreaker,   // Use circuit breaker pattern
    HealthBased,      // Based on health checks
}

#[async_trait]
impl SessionStore for FallbackStore {
    async fn create_session(&self, session: Session) -> SessionResult<()> {
        // Try primary with circuit breaker
        match self.circuit_breaker.call(
            self.primary.create_session(session.clone())
        ).await {
            Ok(()) => Ok(()),
            Err(CircuitError::Open) | Err(CircuitError::CallFailed(_)) => {
                // Log the fallback
                if self.config.log_failures {
                    warn!("Primary storage failed, using fallback for session {}", session.id);
                }
                
                // Alert if configured
                if self.config.alert_on_fallback {
                    self.send_alert("Primary storage unavailable").await;
                }
                
                // Use fallback
                self.fallback.create_session(session).await
            }
        }
    }
    
    async fn get_session(&self, id: &SessionId) -> SessionResult<Session> {
        // Try primary first
        match self.circuit_breaker.call(
            self.primary.get_session(id)
        ).await {
            Ok(session) => Ok(session),
            Err(_) => {
                // Try fallback
                match self.fallback.get_session(id).await {
                    Ok(session) => {
                        // Optionally sync back to primary when it recovers
                        if self.config.sync_on_recovery && self.is_primary_healthy().await {
                            let _ = self.primary.create_session(session.clone()).await;
                        }
                        Ok(session)
                    }
                    Err(e) => Err(e)
                }
            }
        }
    }
    
    // Implement other trait methods similarly...
}
```

### 3. Health Monitor
Location: `shadowcat/src/session/storage/health.rs`

```rust
pub struct HealthMonitor {
    stores: Vec<Arc<dyn SessionStore>>,
    config: HealthConfig,
    health_status: Arc<RwLock<HashMap<String, HealthStatus>>>,
}

pub struct HealthConfig {
    pub check_interval: Duration,
    pub timeout: Duration,
    pub unhealthy_threshold: u32,
    pub healthy_threshold: u32,
}

#[derive(Debug, Clone)]
pub struct HealthStatus {
    pub healthy: bool,
    pub last_check: Instant,
    pub consecutive_failures: u32,
    pub consecutive_successes: u32,
    pub latency_ms: Option<u64>,
    pub error: Option<String>,
}

impl HealthMonitor {
    pub async fn start(self: Arc<Self>) {
        let mut interval = tokio::time::interval(self.config.check_interval);
        
        loop {
            interval.tick().await;
            self.check_all_stores().await;
        }
    }
    
    async fn check_all_stores(&self) {
        for (idx, store) in self.stores.iter().enumerate() {
            let health = self.check_store_health(store).await;
            let store_id = format!("store_{}", idx);
            
            let mut statuses = self.health_status.write().await;
            statuses.insert(store_id, health);
        }
    }
    
    async fn check_store_health(&self, store: &Arc<dyn SessionStore>) -> HealthStatus {
        let start = Instant::now();
        
        // Try a simple operation with timeout
        let check_result = tokio::time::timeout(
            self.config.timeout,
            store.count_sessions()
        ).await;
        
        let latency_ms = start.elapsed().as_millis() as u64;
        
        match check_result {
            Ok(Ok(_)) => HealthStatus {
                healthy: true,
                last_check: Instant::now(),
                consecutive_failures: 0,
                consecutive_successes: 1, // Increment properly in real impl
                latency_ms: Some(latency_ms),
                error: None,
            },
            Ok(Err(e)) | Err(_) => HealthStatus {
                healthy: false,
                last_check: Instant::now(),
                consecutive_failures: 1, // Increment properly in real impl
                consecutive_successes: 0,
                latency_ms: Some(latency_ms),
                error: Some(e.to_string()),
            }
        }
    }
    
    pub async fn is_healthy(&self, store_id: &str) -> bool {
        self.health_status.read().await
            .get(store_id)
            .map(|s| s.healthy)
            .unwrap_or(false)
    }
}
```

### 4. Hybrid Storage Strategy
Location: `shadowcat/src/session/storage/hybrid.rs`

```rust
pub struct HybridStore {
    redis: Arc<dyn SessionStore>,
    memory: Arc<dyn SessionStore>,
    strategy: HybridStrategy,
}

pub enum HybridStrategy {
    WriteThrough,     // Write to both, read from Redis
    WriteBehind,      // Write to memory, async to Redis
    ReadThrough,      // Read from memory, fallback to Redis
    CacheAside,       // Application manages caching
}

impl HybridStore {
    pub async fn new(
        redis: Arc<dyn SessionStore>,
        memory: Arc<dyn SessionStore>,
        strategy: HybridStrategy,
    ) -> Self {
        Self { redis, memory, strategy }
    }
}

#[async_trait]
impl SessionStore for HybridStore {
    async fn create_session(&self, session: Session) -> SessionResult<()> {
        match self.strategy {
            HybridStrategy::WriteThrough => {
                // Write to memory first (fast)
                self.memory.create_session(session.clone()).await?;
                
                // Then write to Redis (can be async)
                tokio::spawn({
                    let redis = self.redis.clone();
                    let session = session.clone();
                    async move {
                        if let Err(e) = redis.create_session(session).await {
                            error!("Failed to write session to Redis: {}", e);
                        }
                    }
                });
                
                Ok(())
            }
            HybridStrategy::WriteBehind => {
                // Write to memory immediately
                self.memory.create_session(session.clone()).await?;
                
                // Queue for later Redis write
                self.queue_redis_write(session).await;
                Ok(())
            }
            _ => self.redis.create_session(session).await,
        }
    }
    
    async fn get_session(&self, id: &SessionId) -> SessionResult<Session> {
        match self.strategy {
            HybridStrategy::ReadThrough => {
                // Try memory first
                match self.memory.get_session(id).await {
                    Ok(session) => Ok(session),
                    Err(_) => {
                        // Fallback to Redis
                        let session = self.redis.get_session(id).await?;
                        // Cache in memory
                        let _ = self.memory.create_session(session.clone()).await;
                        Ok(session)
                    }
                }
            }
            _ => self.redis.get_session(id).await,
        }
    }
}
```

### 5. Integration with SessionManager

Update `SessionManager` to use fallback storage:

```rust
impl SessionManager {
    pub fn with_fallback_store(
        primary: Arc<dyn SessionStore>,
        fallback: Arc<dyn SessionStore>,
        config: FallbackConfig,
    ) -> Self {
        let circuit_breaker = CircuitBreaker::new(CircuitConfig {
            failure_threshold: 5,
            success_threshold: 3,
            timeout: Duration::from_secs(30),
            half_open_requests: 3,
        });
        
        let store = Arc::new(FallbackStore {
            primary,
            fallback,
            circuit_breaker,
            health_monitor: HealthMonitor::new(),
            config,
        });
        
        Self::with_store(store)
    }
}
```

## Testing Strategy

### 1. Circuit Breaker Tests
```rust
#[tokio::test]
async fn test_circuit_breaker_opens_after_failures() {
    let cb = CircuitBreaker::new(CircuitConfig {
        failure_threshold: 3,
        // ...
    });
    
    // Simulate failures
    for _ in 0..3 {
        let _ = cb.call(async { Err::<(), _>("error") }).await;
    }
    
    // Circuit should be open
    assert!(matches!(cb.call(async { Ok(()) }).await, Err(CircuitError::Open)));
}
```

### 2. Fallback Integration Tests
```rust
#[tokio::test]
async fn test_fallback_on_primary_failure() {
    let primary = Arc::new(FailingStore::new()); // Mock that always fails
    let fallback = Arc::new(InMemorySessionStore::new());
    
    let store = FallbackStore::new(primary, fallback, Default::default());
    
    let session = Session::new(SessionId::new(), TransportType::Stdio);
    
    // Should succeed using fallback
    assert!(store.create_session(session.clone()).await.is_ok());
    assert!(store.get_session(&session.id).await.is_ok());
}
```

### 3. Health Monitor Tests
```rust
#[tokio::test]
async fn test_health_monitor_detects_unhealthy_store() {
    let unhealthy = Arc::new(SlowStore::new(Duration::from_secs(10)));
    let monitor = HealthMonitor::new(vec![unhealthy], HealthConfig {
        timeout: Duration::from_millis(100),
        // ...
    });
    
    monitor.check_all_stores().await;
    
    assert!(!monitor.is_healthy("store_0").await);
}
```

## Monitoring & Alerts

### Metrics to Track
```rust
- circuit_breaker_state (gauge: 0=closed, 1=open, 2=half-open)
- fallback_invocations (counter)
- primary_failures (counter)
- health_check_duration (histogram)
- sync_operations (counter)
```

### Alert Conditions
- Circuit breaker open for > 5 minutes
- Fallback used > 100 times in 1 minute
- Health check fails > 10 consecutive times
- Sync queue > 1000 items

## Success Criteria

- ✅ Circuit breaker implementation tested
- ✅ Fallback seamlessly handles Redis failures
- ✅ Health monitoring running continuously
- ✅ No data loss during failover
- ✅ Automatic recovery when Redis returns
- ✅ Performance degradation < 10% with fallback

## Notes

- Consider implementing exponential backoff for retries
- Add metrics for monitoring failover events
- Document failover behavior in operations guide
- Consider implementing read-repair for consistency
- Plan for split-brain scenarios in distributed setup

## Estimated Duration
3 hours

## Dependencies
- Task 2.1 (RedisStore implementation)
- Task 1.1 (SessionStore trait)