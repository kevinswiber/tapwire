# SSE Infrastructure Analysis for Distributed Session Management

## Executive Summary

The existing SSE infrastructure in `shadowcat/src/transport/sse/` provides robust event parsing and reconnection capabilities but **lacks critical distributed session support**. While it has sophisticated local session management, the architecture is **not designed for Redis or distributed backends**. Major refactoring is needed to support the reverse proxy's distributed session requirements.

## Module Capabilities

### 1. SSE Parser (`parser.rs` - 373 lines)
**Purpose**: Incremental SSE event parsing from byte streams
**Key Features**:
- Uses pooled buffers via `global_pools::SSE_POOL` for efficiency
- Handles UTF-8 validation and BOM detection
- Supports incremental feeding with `feed()` and `feed_eof()`
- Max buffer size protection (default 1MB)

**Reusability**: ✅ **Directly usable** for parsing upstream SSE events
```rust
let mut parser = SseParser::new();
let events = parser.feed(&chunk)?;  // Parse incrementally
```

### 2. SSE Buffer (`buffer.rs` - 188 lines)
**Purpose**: Stream wrapper for async SSE parsing
**Key Features**:
- Wraps `AsyncRead` sources with buffering
- Manages pending events queue
- Implements `Stream<Item = SseResult<SseEvent>>`

**Reusability**: ✅ **Directly usable** with `bytes_stream()`
```rust
let stream = response.bytes_stream();
let sse_stream = SseStream::new(StreamReader::new(stream));
```

### 3. Session-Aware Manager (`session.rs` - 607 lines)
**Purpose**: Coordinates SSE connections with MCP sessions
**Critical Issues for Distribution**:
- ❌ Uses in-memory `HashMap` for session storage
- ❌ No abstraction for external storage backends
- ❌ Direct coupling to `InMemorySessionStore`
- ❌ Expiry monitoring assumes local memory access

**Key Structure**:
```rust
pub struct SessionAwareSseManager {
    session_store: Arc<RwLock<HashMap<SessionId, SseSessionState>>>, // In-memory only!
    // ... other fields
}
```

### 4. Reconnection Logic (`reconnect.rs` - 1,477 lines!)
**Purpose**: Handles SSE reconnection with exponential backoff
**Key Features**:
- Last-Event-Id tracking for resumption
- Circular buffer for event deduplication (in-memory)
- Health monitoring and circuit breaker patterns
- Connection state machine

**Distribution Concerns**:
- ❌ Event deduplication buffer is in-memory only
- ❌ Last-Event-Id stored locally, not persisted
- ⚠️ Connection health metrics not shared across instances

### 5. Connection Manager (`manager.rs` - 489 lines)
**Purpose**: Manages multiple SSE connections
**Issues**:
- ❌ Connection registry is local HashMap
- ❌ No support for connection handoff between proxies
- ❌ Load balancing logic assumes single instance

## SessionStore Trait Analysis

### Current State: NO TRAIT EXISTS!
The codebase **does not have a SessionStore trait**. The `InMemorySessionStore` is a concrete struct directly used by `SessionManager`:

```rust
pub struct SessionManager {
    store: Arc<InMemorySessionStore>,  // Direct coupling!
    // ...
}

pub struct InMemorySessionStore {
    sessions: Arc<RwLock<HashMap<SessionId, Session>>>,
    frames: Arc<RwLock<HashMap<SessionId, Vec<MessageEnvelope>>>>,
}
```

### Required Abstraction
We need to create a `SessionStore` trait as planned in `plans/redis-session-storage/`:
```rust
#[async_trait]
pub trait SessionStore: Send + Sync {
    async fn create_session(&self, session: Session) -> SessionResult<()>;
    async fn get_session(&self, id: &SessionId) -> SessionResult<Session>;
    async fn update_session(&self, session: Session) -> SessionResult<()>;
    async fn delete_session(&self, id: &SessionId) -> SessionResult<()>;
    
    // SSE-specific methods needed:
    async fn store_last_event_id(&self, session_id: &SessionId, event_id: String) -> SessionResult<()>;
    async fn get_last_event_id(&self, session_id: &SessionId) -> SessionResult<Option<String>>;
    async fn buffer_sse_event(&self, session_id: &SessionId, event: SseEvent) -> SessionResult<()>;
    async fn get_buffered_events(&self, session_id: &SessionId, since: Option<String>) -> SessionResult<Vec<SseEvent>>;
}
```

## Session Mapping Architecture

### Current Limitations
1. **No Dual Session IDs**: System assumes single session ID throughout
2. **No Upstream Mapping**: Cannot map proxy sessions to upstream sessions
3. **No Connection Pooling**: Each client gets dedicated upstream connection

### Required for Reverse Proxy
```rust
struct ProxySessionMapping {
    proxy_session_id: SessionId,      // Generated by proxy
    upstream_session_id: Option<String>, // From upstream's Mcp-Session-Id
    client_info: ClientInfo,
    upstream_connection: Arc<UpstreamConnection>, // Pooled/shared
}
```

## Event Buffering & Backpressure

### Current Event Buffering
- **In-Memory Only**: Events stored in `Vec<SseEvent>` 
- **No Persistence**: Lost on proxy restart
- **No Size Limits**: Could cause memory issues
- **Local Deduplication**: Won't work across instances

### Minimal Buffering Requirements
Based on MCP spec and TypeScript SDK analysis:
- **Last-Event-Id Only**: Just need to track last successfully sent event ID
- **No Full Replay**: Upstream should handle replay from Last-Event-Id
- **Bounded Buffer**: If buffering, limit to last N events or time window

### Backpressure Strategy
The proxy should be a **thin pipe**, not a reservoir:
```rust
// Good: Stream through with minimal buffering
let mut upstream_stream = response.bytes_stream();
while let Some(chunk) = upstream_stream.next().await {
    let events = parser.feed(&chunk)?;
    for event in events {
        // Process through interceptors
        let modified = interceptor_chain.process_sse_event(event).await?;
        // Stream immediately to client
        client_writer.write_sse_event(modified).await?;
    }
}
```

## Gap Analysis

### Critical Gaps for Distributed Sessions

1. **No Storage Abstraction**
   - Need: `SessionStore` trait
   - Current: Direct `InMemorySessionStore` coupling
   - Impact: Cannot add Redis backend without major refactor

2. **No Session Mapping**
   - Need: Dual session ID tracking
   - Current: Single session ID throughout
   - Impact: Cannot maintain proxy sessions across upstream changes

3. **Local Event Buffers**
   - Need: Distributed event buffer or Last-Event-Id only
   - Current: In-memory Vec buffers
   - Impact: Reconnection fails across proxy instances

4. **Missing Connection Pooling**
   - Need: Shared upstream connections
   - Current: 1:1 client-to-upstream mapping
   - Impact: Resource waste, no failover

5. **No Distributed Metrics**
   - Need: Shared health/rate limit state
   - Current: Local memory counters
   - Impact: Can't coordinate across proxies

## Integration Points with UpstreamResponse

### Using SseParser with bytes_stream()
```rust
async fn stream_sse_with_interceptors(
    upstream: UpstreamResponse,
    interceptor_chain: Arc<InterceptorChain>,
    session_store: Arc<dyn SessionStore>,  // Distributed store
) -> Result<Response> {
    let mut parser = SseParser::new();
    let mut stream = upstream.response.bytes_stream();
    
    // Get or create proxy session mapping
    let mapping = session_store.get_or_create_mapping(...).await?;
    
    // Stream SSE with distributed session support
    let (tx, body) = Body::channel();
    
    tokio::spawn(async move {
        while let Some(chunk) = stream.next().await {
            let chunk = chunk?;
            let events = parser.feed(&chunk)?;
            
            for event in events {
                // Store Last-Event-Id for reconnection
                if let Some(ref id) = event.id {
                    session_store.store_last_event_id(&mapping.proxy_session_id, id).await?;
                }
                
                // Process through interceptors
                let modified = interceptor_chain.process_sse_event(event).await?;
                
                // Send to client immediately (minimal buffering)
                tx.send_data(format_sse_event(modified)).await?;
            }
        }
    });
    
    Ok(Response::builder()
        .status(200)
        .header("Content-Type", "text/event-stream")
        .header("Cache-Control", "no-cache")
        .body(body)?)
}
```

## Recommendations

### Phase 1: Storage Abstraction (Prerequisite)
1. **Extract SessionStore trait** from InMemorySessionStore
2. **Add SSE-specific methods** for Last-Event-Id and event buffering
3. **Implement Redis backend** with the trait

### Phase 2: Reusable Components
**Use As-Is**:
- `SseParser` for parsing upstream events
- `SseStream` for wrapping bytes_stream()
- `SseEvent` and `EventBuilder` structures

**Modify Heavily**:
- `SessionAwareSseManager` - Refactor to use SessionStore trait
- `ReconnectingStream` - Add distributed Last-Event-Id support

**Replace/Skip**:
- Connection registry - Use distributed session store
- In-memory event buffers - Use Redis or remove entirely

### Phase 3: New Components Needed
1. **ProxySessionMapper**: Manages dual session IDs
2. **DistributedEventBuffer**: Redis-backed event buffer (if needed)
3. **SseInterceptorAdapter**: Integrate SSE events with interceptor chain
4. **UpstreamConnectionPool**: Share upstream connections

### Implementation Strategy

1. **Minimal Buffering First**: Start with Last-Event-Id only
   - Store just the last event ID in Redis
   - Let upstream handle replay logic
   - Add buffering only if upstream doesn't support Last-Event-Id

2. **Thin Pipe Architecture**: 
   - Stream events immediately after processing
   - Don't accumulate events in memory
   - Use bounded buffers if necessary

3. **Session Mapping Table**:
   ```rust
   // Redis keys:
   proxy:session:{proxy_id} -> upstream_session_id
   upstream:session:{upstream_id} -> Set<proxy_session_id>
   session:last-event:{proxy_id} -> event_id
   ```

4. **Connection Pooling**:
   - Pool upstream connections by upstream URL
   - Map multiple proxy sessions to single upstream connection
   - Handle upstream failover transparently

## Critical Decisions Required

1. **Event Buffering Depth**: How many events to buffer for replay?
   - Recommendation: Start with Last-Event-Id only
   - Add bounded buffer if needed (last 100 events or 5 minutes)

2. **Session Storage Backend**: Redis vs PostgreSQL vs DynamoDB?
   - Recommendation: Redis for low latency and TTL support

3. **Failover Behavior**: What happens when upstream dies?
   - Recommendation: Transparent failover with session migration

4. **Backpressure Strategy**: How to handle slow clients?
   - Recommendation: Drop connection if client too slow (configurable)

## Conclusion

The existing SSE infrastructure provides excellent **local** SSE handling but requires significant refactoring for distributed session management. The parser and stream components are reusable, but the session management layer needs a complete overhaul to support Redis backends and proxy session mapping.

**Critical Path**:
1. Implement SessionStore trait (blocking everything else)
2. Refactor SessionManager to use trait
3. Add Redis backend implementation
4. Modify SSE components for distributed operation
5. Implement proxy session mapping

Without these changes, the reverse proxy cannot support:
- Multiple proxy instances
- Session persistence across restarts  
- SSE reconnection after failover
- Efficient connection pooling